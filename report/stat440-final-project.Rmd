---
title: "Stat440 Final Project"
subtitle: "Dynamical Analysis of Molecular Interactions"
output: pdf_document
author: Karmei Koo (kmkoo) Wanxin Li (w328li) Jayden Luo (j57luo) Yi Xiang (y25xiang)
bibliography: ["ref.bib"]
biblio-style: "apalike"
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Abstract
Fluorescence resonance energy transfer (FRET) has been used to study biological structures and monitor the activities of molecules [@lemke2016forster]. However, some conformational changes are difficult to detect using ensemble FRET. In order to understand the interaction at single molecule level, single-molecule fluorescence resonance energy transfer (smFRET) is developed to probe the detailed kinetics of structure changes within a single molecule [@ha2001single]. By using this revolutionized technique, scientists unlock the opportunities to research more about protein folding-unfolding, protein conformation dynamics, ion channel dynamics, receptor-ligand interactions, nucleic acid structure and conformation, vesicle fusion, and force induced conformational changes [@sasmal2016single]. All the new discoveries would reveal the mechanism of diseases at the cellular level [@ideker2008protein]. Current smFRET research employs Brownian Motion (BM) in modelling free diffusion state [@wallace2017forster] and Ornstein-Uhlenbeck (OU) process to model the bonding state where there is a consistent bond with stochastic perturbations between donor and acceptor [@yang2002single]. One of the remaining challenges of smFRET modelling is whether it is possible to differentiate between free diffusion and bound donor-acceptor pairs. Another challenge is whether we can detect binding and unbinding events. To do so, a simplified experiment setup as follows. The number of photons at time follows a distribution: 
\begin{equation} \label{eq:1} 
Y_n \stackrel{ind}{\sim} Poisson(exp(\beta_0+\beta_1 X_t))
\end{equation} 

where $X_t$  is the true donor-acceptor distance modelled by BM and OU process.

The challenges of this problem are the following:
\begin{itemize}
\item Challenge 1: How to set up experiments so that OU parameters can be accurately estimated
\item Challenge 2: Investigate on whether and if applicable, when it is possible to identify the simulation model by combining parameter inferences and model selection criteria.
\end{itemize}

By a simulation study, our contributions include:
\begin{itemize}
\item Implemented paramter inferences for BM and OU models and tested likelihoods and estimates by unit tests
\item Found a set of $\beta's$ to accurately estimate OU parameters and explained how $\beta's$ affect OU parameter estimation
\item Found a threshold to distinguish BM simulated data from OU model, vice versa, and explained why the thresholds are reasonable mathematically
\item Enabled random start in optimization algorithms
\end{itemize}

# Introduction
## Brownian Motion  
BM is widely used to model the donor-acceptor distance during free diffusion in the molecular environment.The random flow of molecular motion is subtained by collision of moving molecules. In our dynamical analysis, $X_{t}$ is denoted as the donar-acceptor distance at time t. It is a Gaussian Markov process with the following transition density:

\begin{equation} \label{eq:2} X_{t+\Delta t}|X_{t} \sim N(X_{t}, \sigma^2 \Delta t) \end{equation}  

Every subsequent variation in distance by an increment of $\Delta t$ has a mean-reverting nature. That is, the donor-acceptor distance in the next instance revolves around the mean of the process in various direction. In BM model, the mean is set as the current-time donor-aceptor distance. With the process involving Markov element, the sequence of possible molecular motions is only current-state dependent, and it is often refered to as memoryless. The collection of the independent processes at each instance $\Delta t$, are recognised as jointly Gaussian (normal).  

The variation is determined by the diffussion rate $\sigma$, multiplied by $\Delta t$. The diffussion rate is set constant in our study, which implies the thermal temperature and pressure are kept unchanged and no external force is applied into the system.  

However, the many-body interactions that yield the random pattern cannot be solved by BM model that accounts every involved molecule. Therefore, BM itself is not capable to model the full motion of molecules especially during the photon exchange between donor and acceptor. This is ultimately a downside of BM model in our dynamical analysis. 

### Ornstein-Uhlenbeck Process 
Ornstein-Uhlenbeck process is further developed from BM by L. S. Ornstein and G. E. Uhlenbeck.[1]
The OU process is a stochastic Gaussian process with continuous paths. The OU process is defined as the following stochastic differential equation: 
\begin{equation} \label{eq:3} dX_t = \gamma (\mu - X_t) dt + \sigma dW_t \end{equation} 
Where $W_t$ is a standard BM on $t \in (0, \infty),\gamma > 0, \sigma > 0$
$X_t$ is a stationary Gaussian Markov process with transition density: 
\begin{equation} \label{eq:4} X_{t+\Delta t}|X_{t} \sim N(\mu + \omega_{\Delta t} (X_{t} - \mu), \tau^2(1- \omega_{\Delta t}^2)) \end{equation} 
where $\omega_{\Delta t} = exp(-\gamma \Delta t)$ and $\tau^2 = \sigma^2/(2 \gamma)$.
Compared to BM model, the OU process includes more factors from the environment that can impact the change of the distance. The OU process has four components to describe the molecule interactions which are $\gamma$, $\mu$, $\sigma$, and $dW_t$.The interpretation of $\gamma$ is the rate of mean reversion. The OU process shares mean reversion nature with BM. This property allows that the distance will eventually revert to the long-run average.The rate indicates how strong the distance will react toward the attractor. $\mu$ is the asymptotic mean of "bond" length. $\sigma$ is the parameter showing the volatility of noise. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4898467/). The OU process represents how molecule move towards the attractors $\mu$ with BM. 

The OU process considers the current state and the speed of molecule interaction, which is preferred when modeling the many-body interactions.Since with different type of donor-acceptor interactions, the molecule will react differently. 
One of the downsides of OU process is that a very small amount of variation such as measurement error can profoundly affect the performance of the model. Also the distance is always positive. However, the donor acceptor in OU allows negative distance. 

# Methodology

## Laplace Implementation
Using MLE, we aim to estimate the $\hat{\theta}$ that maximize $l(\theta|X, Y)$, which can be approximiated by
$$ l(\theta|X, Y)\approx l_{LAP}(\theta|Y) = l(\theta|\hat{X_{\theta}}, Y)-1/2*log|H_{\theta}|$$
$l_{Lap}(\theta|Y)$. $\hat{\theta}$ is dependent upon $\hat{X_{\theta}}$. In addition, in $X_{\theta}= \text{argmax}_{X} l(\theta|X, Y)$, we need the value of $X_{\theta}$ for $\hat{\theta}$, thus $X_{\theta}$ and $\theta$ have to be optimized simultanously; we use Newton's method to in Laplace approximation to achieve this.

Also, we compare the results of our self-implementated Laplace approximation with TMB's built-in Laplace approximation. Despite of similar accuracies as demostrated in Appendix, the built-in Laplace implementatnion is much faster than ours. Thus, we decide to work with the built-in implementation.

## Multi-start Optimization
We initially implemented our likelihood function under the OU model in TMB using $(\mu, \sigma, \gamma)$ as parameters. However, from our experiments we found that using the R method `optim()` does not satisfiable inference result regardless of the choice of method e.g. BFGS and Nelder-Mead. In addition, we noticed that the passing different initial parameter values  to `optim()` can caused very different inference results. Therefore we implemented a multi-start on top of `optim()`. We believed that performing multi-start with a parameter that has a lower bound and upper bound such as $\omega$ is more effective as the boundary allow us to find maximum gap between different starting points. Therefore we implemented another likelihood implementation using $(\omega, \sigma, \tau)$ as parameters. Below is a RMSE comparison between single start and multi-start.
```{r}
multistart_result <-read.csv("multi_start_result.csv")
multistart_result[, c("num_multistart","rmse.omega","rmse.mu","rmse.tau",
                      "rmse.gamma","rmse.t","rmse.sigma")]
```

## NA Handling
For Challenge 2, whenever NA is simulated in $Y_t$ due to large $\lambda$ in $rpois$, we disgard it together with corresponding $X_t$.

## Simulation for Challenge 1
The goal here is to investigate on how the value of $(\beta_0, \beta_1)$ could affect the accuracy of inference under the OU model. We setup a experiment with true parameters being $\mu=10$, $\tau=1$ and $\gamma \in \{0.1,1,10\}$. In order to evaluate the accuracy of inference, we simulate 100 dataset for each pair of $(\beta_0, \beta_1)$ and calculated the root mean-squared error(RMSE) of $t_{dec}$, $\mu$ and $\tau$ as measure of accuracy. We first find a initial pair of $(\beta_0, \beta_1)$ such that the RMSE reasonably good. We looked into the simulated dataset when $\gamma=1$, shown below. We observed that the values of $X_n$ are mostly below $13$ and hence set a initial $\beta_1=1$ and $\beta_0>13$ such that we have $\beta_0-\beta_1 X_n > 0$ and $Y_n \sim \text{Poisson}(\exp(\beta_0-\beta_1X_n))$ are more likely to have a large set of values. For $\gamma=1$, we decided to simulate $299$ observations per dataset. For $\gamma=0.1$, since we were not able to get good RMSE with $299$ observations per dataset, we increased to $999$ per dataset. We then perform inference simulations with one of the the $\beta$($\beta_0$ or $\beta_1$) fixed and the other gradually decreasing and observe how RMSE changes.
```{r, echo=FALSE}
# TODO: use library
ou_sim <- function(gamma, mu, sigma, dt, n_obs, x0=mu) {
  tau <- sigma/sqrt(2*gamma) # stationary standard deviation
  if(missing(x0)) x0 <- rnorm(1, mean = mu, sd = tau)
  # generate efficiently using a one-step linear filter
  lrho <- -gamma * dt
  ou_sd <- tau * sqrt((1-exp(2 * lrho))) # conditional sd
  ou_filt <- exp(lrho) # filter coefficients
  z <- rnorm(n_obs, sd = ou_sd) # pre-generate normal draws
  Xt <- filter(x = z, filter = ou_filt,
               method = "recursive", init = x0 - mu)
  return(as.numeric(Xt + mu))
}

n_obs = 999
X <- ou_sim(gamma=1, mu=10, sigma=sqrt(2), dt=1, n_obs=999)
plot(1:n_obs, X, main="Xt under OU process")
```

## Simulation for Challenge 2
To start, we simulate from OU process. We perform the experiment 20 times for a fixed set of $\beta_0, \beta_1, \gamma, \mu$. 200 $X_t's$ and $Y_t's$ without NA are generated in each experiment, and paramter inferences are performed. Parameter inferences are performed using "Nelder-Mead" optmization. Similary, we simulate from BM process. We perform the experiment 20 times for a fixed set of $\beta_0, \beta_1$. 400 $X_t's$ and $Y_t's$ without NA are generated in each experiment. Parameter inferences are performed using "BFGS" optimzation. Our setups are as follows:
OU setup, optimization method is "Nelder Mead", number of observation is 100.
\begin{itemize}
\item setup 1: $\beta_0=10, \beta_1=0.5, \tau=1, \mu=0$
\item setup 2: $\beta_0=5, \beta_1=0.5, \tau=2, \mu=0$
\item setup 3: $\beta_0=5, \beta_1=1, \tau=2, \mu=0$
\item setup 4: $\beta_0=10, \beta_1=0.5, \tau=2, \mu=1$
\end{itemize}

And BM setup, optimization method is "BFGS, number of observation is 400.
\begin{itemize}
\item setup 1: $\beta_0=10, \beta_1=0.5, \mu=0, \gamma=5$
\item setup 2: $\beta_0=5, \beta_1=0.5, \mu=0, \gamma=5$
\item setup 3: $\beta_0=5, \beta_1=1, \mu=0, \gamma=5$
\end{itemize}

# Results
We present and analyze our results with respect to two challenges as follows.

## Challenge 1
The result of our simulations are as follows:
```{r}
sim1_result <- read.csv("sim1_result.csv")
sim1_result <- sim1_result[sim1_result["num_multistart"]==10,]
show_col <- c("beta0", "beta1", "n_obs", "theta.mu", "theta.t", "theta.tau",
              "rmse.mu", "rmse.t", "rmse.tau")
sim1_result <- sim1_result[with(sim1_result, order(-beta1, -beta0)),]
```

### gamma=1

Below is the simulation result with $\gamma=1$. We were able to get $\text{RMSE}_\theta<1\%$ and $\text{RMSE}_\tau<10\%$ but $\text{RMSE}_t<20\%$ only. The result shows that when $\beta_1=1$ the minimum value of $\beta_0$ to maintain a good RMSE is between $11$ and $12$. The result shows that when $\beta_0=15$ the minimum value of $\beta_1$ to maintain a good RMSE is between $0.8$ and $1$. 
```{r}
signif(sim1_result[sim1_result[,"gamma"]==1, show_col], 3)
```

### gamma=0.1

The simulation result with $\gamma=1$ is shown below. In this setup, we were able to get $\text{RMSE}_\theta<5\%$ and $\text{RMSE}_\tau<10\%$ but $\text{RMSE}_t<20\%$ only. Thee result shows that our model were not able to achieve good accuracy when there are only $299$ observations in the dataset. However, with the number of observations being $999$, the model were able to get much better $RMSE$. In this setup, the minimum value of $\beta_0$ for good accuracy is between $9$ and $11$ when $\beta_1=1$, and the minimum value of $\beta_1$ for good accuracy is between $0.9$ and $1$.
```{r}
signif(sim1_result[sim1_result[,"gamma"]==0.1, show_col], 3) 
```

### gamma=10
For $\gamma=10$, as shown below, we were able to get $\text{RMSE}_\theta<1\%$ and $\text{RMSE}_\tau<10\%$ but not able to estimate parameter $t$ nicely even with $999$ observations per dataset. The lower bound for $\beta_0$ for a good $\text{RMSE}_\tau$ is between $11$ and $12$ and the lower bound for $\beta_1$ for good $\text{RMSE}_\tau$ is between $0.6$ and $0.8$.
```{r}
signif(sim1_result[sim1_result[,"gamma"]==10, show_col], 3) 
```


## Challenge 2
For $X_t$ simulated from OU model, as shown below, for 4 setups, the probablities to pick OU model by AIC increase as $\gamma$'s go up from 0.01 to 4.
```{r}
sim2_OU_result <- read.csv("sim2_table1.csv", check.names=FALSE)
print(sim2_OU_result,row.names = FALSE)
```
 
For $X_t$ simulated from BM model, as shown below, for 4 setups, the probablities to pick BM model by AIC increase as $\sigma$'s go up from 1e-6 to 2. However, when $\sigma = 3$, the probabilities goes down.
```{r}
sim2_BM_result <- read.csv("sim2_table2.csv", check.names=FALSE)
print(sim2_BM_result,row.names = FALSE)
```

# Discussion

## Other Potential Models for Molecular Dynamics Study

### Morse Interaction Model

Morse Interaction model[@schelstraete1999energy] is described as a combination of BM and improved OU process.
Under this model, $X_{t}$ is a Markov process satisfying the stochastic differential equation (SDE):
$$
dX_{t}=-U'(X_{t})dt+\sigma dB_{t}
$$

And $U'(x)$ is the derivative of the Morse potetial energy function: 
$$
U'(x)=\gamma \centerdot(1-e^{-\alpha \centerdot (x-u)})
$$
The distance $X_{t}$ is set to be strictly positive, $X_{t}$ > 0. When $X_{t}$ is too large, repulsive forces allow bond breaking to occur - the molecules again resemble BM. 
When the donor and acceptor again get close to each other, the bond is reformed and exchange of photons takes place. The dynamics then resembles OU process.

Morse interaction model is a comprehensive approach towards dynamics study of molecular interaction. 

  
### Lennard-Jones Potential
The interaction between two non-bonded and un-charged atoms, known as Van der Waals interaction, has been expressed in terms of potential energy. Lennard-Jones potential[@libretexts_2019] is probably the most famous pair potential descriping interatomic Van der Waals forces. It consists of two parts:  
1) A steep repulsive term; and  
2) A smooth attractive term

$$
V(r)=4\epsilon [\left(\frac{\sigma}{r}\right)^{12}-\left(\frac{\sigma}{r}\right)^{6}]
$$  
In particular, $V(x)$ is the intermolecular potential between the two molecules, and $r$ is the distance of separation between both molecules.  

Apart from being a widely used model itself, Lennard-Jones potential also sometimes forms one of 'building blocks' of many force fields, due to its computational expediency. 

However, Lennard-Jones potential is not designated to model donor-acceptor interactions.


## Challenge 1

### Lower Bounds of beta0
Under $\mu=10, \tau=1$, we simulated 1000 datasets and find the mean and range of the simulated $X_n$ datasets as below:
```{r, echo=FALSE}
# TODO: use ou_sim from package
ou_sim <- function(gamma, mu, sigma, dt, n_obs, x0=mu) {
  tau <- sigma/sqrt(2*gamma) # stationary standard deviation
  if(missing(x0)) x0 <- rnorm(1, mean = mu, sd = tau)
  # generate efficiently using a one-step linear filter
  lrho <- -gamma * dt
  ou_sd <- tau * sqrt((1-exp(2 * lrho))) # conditional sd
  ou_filt <- exp(lrho) # filter coefficients
  z <- rnorm(n_obs, sd = ou_sd) # pre-generate normal draws
  Xt <- filter(x = z, filter = ou_filt,
               method = "recursive", init = x0 - mu)
  return(as.numeric(Xt + mu))
}

sapply(c(0.1,1,10), function(gamma) {
  ou_X <- ou_sim(gamma=gamma, mu=10, sigma=sqrt(2*gamma), dt=1, n_obs=999)
  c(gamma=gamma, min=min(ou_X), mean=mean(ou_X), max=max(ou_X))
  })
```
With $\beta_1=1$, when $\beta_0$ drops under $13$, $\beta_0 - \beta_1 X_n$ has a probability of becoming negative. As $\beta_0$ continue to decrease, the probability to become negative increases. When $\beta_0 - \beta_1 X_n$ is negative and $\exp(\beta_0 - \beta_1 X_n)<1$, the $Y_n \sim \text{Poisson}(\exp(\beta_0-\beta_1 X_n))$ generate only few values, with large probability being either $0$ or $1$. This means that the characteristics of $X_n$ datasets are not reflected to $Y_n$ and inference on $X_n$ parameters using $Y_n$ data will perform poorly.

### Lower Bounds of beta1
To understand why the value of $\beta_1$ affect inference result, we simulated $100$ $X_n$ and $Y_n$ datasets of size $299$ and computed their corresponding $p(Y|X)$.
```{r, echo=FALSE}
# TODO: use package
y_sim <- function(X, beta0, beta1) {
  sapply(X, function(x) rpois(1, exp(beta0-beta1*x)))
}
ou_sim <- function(gamma, mu, sigma, dt, n_obs, x0=mu) {
  tau <- sigma/sqrt(2*gamma) # stationary standard deviation
  if(missing(x0)) x0 <- rnorm(1, mean = mu, sd = tau)
  # generate efficiently using a one-step linear filter
  lrho <- -gamma * dt
  ou_sd <- tau * sqrt((1-exp(2 * lrho))) # conditional sd
  ou_filt <- exp(lrho) # filter coefficients
  z <- rnorm(n_obs, sd = ou_sd) # pre-generate normal draws
  Xt <- filter(x = z, filter = ou_filt,
               method = "recursive", init = x0 - mu)
  return(as.numeric(Xt + mu))
}

vec_beta1 <- seq(0.1,1,by=0.1)
loglik <- sapply(vec_beta1, function(beta1) {
  beta0 <- 15
  X <- ou_sim(gamma=1, mu=10, sigma=sqrt(2), dt=1, n_obs=299)
  Y <- y_sim(X, beta0=beta0, beta1=beta1)
  sum(dpois(Y,exp(beta0-beta1*X), log=TRUE))
})
rbind(beta1=vec_beta1, loglik=loglik)
```
This shows that under the same parameter and $\beta_0$, different $\beta_1$ can have different loglikelihood. From this observation, we further investigated into $l(\theta|X,Y) = log\{p(Y|X) \times p(X|\theta)\}$ and found that changes in $\beta_1$ affect the term $\sum_{n=0}^N Y_n (\beta_0-\beta_1 X_n)=\sum_{n=0}^N Y_n\beta_0 - \beta_1X_nY_n$. Specifically, if we consider $\beta_1$ as a weight for the term $X_nY_n$, then decreasing $\beta$ is reducing the impact of the term $X_nY_n$ on $l(\theta|X,Y)$. We notice that this is the only term $Y_n$-related term in finding $\hat{X_\theta}$ in laplace approximation. Therefore, in finding $hat{X_\theta}$, decreasing $\beta_1$ acts as decreasing the importance of the value of $Y_n$. We suspect that this is the reason why $\beta_1$ has a lower bound. However, even if this is the case, it is difficult to determine what the lower bound is without performing simulation. 

### Why `n_obs` has Impact on RMSE when gamma=0.1
When $\gamma=0.1$, equation 2 is aprroximatly $X_{t+\Delta t} | X_t \sim N(0.1\mu+0.9X_t, 0.2\tau)$, which means that the value of $X_t$ has plays important factor in the likelihood of $X_{t+\Delta t}$. Since $X_t$ are not given and only approximated using laplace, inference would depend more on Laplace approximation. Since $X_t$ are approximated using MLE and MLE methods are generally affected number of observations, we suspect that the performance of Laplace approximation in this model would also depend on dataset size. 

## Challenge 2
### OU Simulation
For experiments simulated from OU, we can see a consistent improvement in picking the correct model when $\gamma$ goes up. As $\gamma\rightarrow 0$, $\omega_{\Delta{t}}\rightarrow 1$ and $\tau$ is still a constant, equation \ref{eq:4} becomes $X_{t+\Delta{t}}|X_t \sim N(X_t, 0)$, which is in the same form as equation \ref{eq:2}. Thus, it is difficult to distinguish the OU simulation from the BM model. As $\gamma$ goes up, $\omega_{\Delta{t}}\rightarrow 0$, \ref{eq:2} and \ref{eq:4} differ in mean and variance, and hence become easier to distinguish.

### BM Simulation
For experiments simulated from BM, we can see a consistent improvement in picking the correct model when $\sigma_{BM}$ goes up, however, the improvement stops at $\sigma_{BM}=3$ for most of the experiments. We analyze in the following two aspects. First of all, when $\sigma_{BM} \rightarrow 0$, equation \ref{eq:2} becomes $X_{t+\Delta{t}}|X_t \sim N(X_t, 0)$. From the discussion in the previous paragraph, when $\gamma \rightarrow 0$, equation \ref{eq:4} also becomes $X_{t+\Delta{t}}|X_t \sim N(X_t, 0)$. Thus, if we do parameter inferences are done correctly, we can also find OU parameters that fit the data well. It will be difficult to tell the BM simulated data from OU models. Furthermore, when $\sigma$ becomes too large with respect to $\beta_0$ and $\beta_1$, for example, when $\sigma=3$ with respect to $\beta_0=5$ and $\beta_1=1$, the probability of generating large $X_t$ becomes higher. In this case, $exp(\beta_0 -\beta_1 X_t)$ becomes close to 0, and the generated $Y_t$'s from Poission equation \ref{eq:1} are more likely to be 0's. Consequently, $Y_t$ does not provide enough information to the underlying generating process, and hence difficult to tell apart BM model from OU model.

Another observation to note is that as a general rule, $|\beta_0-\beta_1|$ cannot be too large; otherwise, many NAs from equation \ref{eq:1} will be generated because R cannot handle values $> e^{22}$. Referring to how we handle NAs in the Methodology section, repetively generating values until no NAs results will result in huge computation cost.

## Potential Improvements in Implementation
### Other Model Selection Criterion 
Only AIC is used in model selection for Challenge 2. We can consider other model selection criteria for Challenge 2. For example, AIC and Mallow CP. Using a weighted of different criteria for model selection makes the result less biased. 

### Random Start for BM model
As discussed in the methodology section, random. We figured out how to implement random start for OU model by reparameterization. Consequently, the accuracies of OU inferences have been improved sigfinicantly. It will be useful to figure out similar approaches for BM model.

### R Drawback
One of the challenges encountered in the project is the efficiency of running simulation results. Each simulation with different parameters took over 1 hour to obtain the final results. This exposes one of the disadvantages of R, which is that R does not support running test cases in parallel. 

\newpage
# Appendix

\newpage
# References
